{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cool Roofs Detection Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "import shutil\n",
    "import rasterio\n",
    "import shapefile\n",
    "from shapely.geometry import shape, Polygon, MultiPolygon\n",
    "import yaml\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define functions for creating a custom dataset\n",
    "These functions create a custom dataset that follows the format required for training a YOLO segmentation model.\n",
    "```\n",
    "dataset\n",
    "└───images\n",
    "│   └───test\n",
    "│       │   img1.jpg\n",
    "│       │   img2.jpg\n",
    "│       │   ...\n",
    "│   └───train\n",
    "|       |   ...\n",
    "|   └───validate\n",
    "|       |   ...\n",
    "|\n",
    "└───labels\n",
    "    └───test\n",
    "        │   img1.txt\n",
    "        │   img2.txt\n",
    "        │   ...\n",
    "    └───train\n",
    "        |   ...\n",
    "    └───validate\n",
    "        |   ...\n",
    "```\n",
    "In the example dataset above, the test image `img1.jpg` could have any number of roofs. The extents of those roofs are found in `img1.txt` in the corresponding folder for test labels and are represented as coordinates corresponding to pixels in the image. A sigle row in the label file represents one roof and is formatted as `<class-index> <x1> <y1> <x2> <y2> ... <xn> <yn>`, where `<class-index>` is either 0 for warm roofs or 1 for cool roofs and is followed by the roof's coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_structure(output_folder):\n",
    "    \"\"\"Create the dataset directory structure\"\"\"\n",
    "    splits = ['train', 'val', 'test']\n",
    "    types = ['images', 'labels']\n",
    "    \n",
    "    for type_dir in types:\n",
    "        for split in splits:\n",
    "            path = Path(output_folder) / 'segment' / type_dir / split\n",
    "            path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Create the YAML configuration file for the dataset\n",
    "    yaml_content = {\n",
    "        'path': '',\n",
    "        'train': 'images/train',\n",
    "        'val': 'images/val',\n",
    "        'test': 'images/test',\n",
    "        'names': {\n",
    "            0: 'warm_roof',\n",
    "            1: 'cool_roof'\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    yaml_path = Path(output_folder) / 'segment' / 'segment.yaml'\n",
    "    with open(yaml_path, 'w') as f:\n",
    "        yaml.safe_dump(yaml_content, f, sort_keys=False)\n",
    "\n",
    "    return splits\n",
    "\n",
    "def assign_split(splits, split_ratios):\n",
    "    \"\"\"Assign a split based on the provided ratios\"\"\"\n",
    "    rand_val = random.random()\n",
    "    cumulative = 0\n",
    "    for split, ratio in zip(splits, split_ratios):\n",
    "        cumulative += ratio\n",
    "        if rand_val <= cumulative:\n",
    "            return split\n",
    "    return splits[-1]  # Fallback to last split\n",
    "\n",
    "def coords_to_pixel(x, y, transform, img_width, img_height):\n",
    "    \"\"\"Convert coordinates to pixel row and column with bounds checking\"\"\"\n",
    "    col, row = ~transform * (x, y)\n",
    "    \n",
    "    # Normalize and clamp coordinates to [0,1]\n",
    "    col = max(0, min(1, col / img_width))\n",
    "    row = max(0, min(1, row / img_height))\n",
    "\n",
    "    return col, row\n",
    "\n",
    "def validate_polygon(points, img_width, img_height):\n",
    "    \"\"\"\n",
    "    Validate polygon coordinates and ensure they are within image boundaries\n",
    "    Returns: (is_valid, message)\n",
    "    \"\"\"\n",
    "    if not points or len(points) < 3:  # Need at least 3 points for a polygon\n",
    "        return False, \"Too few points for polygon\"\n",
    "    \n",
    "    # Check if all coordinates are within [0,1] range\n",
    "    for x, y in points:\n",
    "        if not (0 <= x <= 1 and 0 <= y <= 1):\n",
    "            return False, f\"Coordinates out of bounds: ({x}, {y})\"\n",
    "    \n",
    "    # Check if polygon has non-zero area\n",
    "    # Convert normalized coordinates back to pixels for area calculation\n",
    "    pixel_points = [(x * img_width, y * img_height) for x, y in points]\n",
    "    polygon = Polygon(pixel_points)\n",
    "    if polygon.area < 1:  # Area less than 1 pixel\n",
    "        return False, \"Polygon area too small\"\n",
    "        \n",
    "    return True, \"Valid polygon\"\n",
    "\n",
    "def process_geometry(geom, transform, img_width, img_height):\n",
    "    \"\"\"\n",
    "    Process a geometry (Polygon or MultiPolygon) and return list of valid coordinate strings\n",
    "    \"\"\"\n",
    "    valid_polygons = []\n",
    "    \n",
    "    if isinstance(geom, MultiPolygon):\n",
    "        # Process each part of the MultiPolygon separately\n",
    "        for part in geom.geoms:\n",
    "            if isinstance(part, Polygon):\n",
    "                points = list(part.exterior.coords)\n",
    "                pixel_points = [coords_to_pixel(x, y, transform, img_width, img_height) \n",
    "                              for x, y in points]\n",
    "                \n",
    "                is_valid, message = validate_polygon(pixel_points, img_width, img_height)\n",
    "                if is_valid:\n",
    "                    coord_str = ' '.join(f\"{x} {y}\" for x, y in pixel_points)\n",
    "                    valid_polygons.append(coord_str)\n",
    "    \n",
    "    elif isinstance(geom, Polygon):\n",
    "        points = list(geom.exterior.coords)\n",
    "        pixel_points = [coords_to_pixel(x, y, transform, img_width, img_height) \n",
    "                       for x, y in points]\n",
    "        \n",
    "        is_valid, message = validate_polygon(pixel_points, img_width, img_height)\n",
    "        if is_valid:\n",
    "            coord_str = ' '.join(f\"{x} {y}\" for x, y in pixel_points)\n",
    "            valid_polygons.append(coord_str)\n",
    "    \n",
    "    return valid_polygons\n",
    "\n",
    "def create_dataset(image_folder, shapefile_folder, output_folder, split_ratios=(0.7, 0.2, 0.1)):\n",
    "    \"\"\"\n",
    "    Create a custom dataset by converting shapefiles to YOLO segmentation format using 'cool_roof'\n",
    "    field as class_id and split into train/val/test sets\n",
    "    \"\"\"\n",
    "    # Validate split ratios\n",
    "    if len(split_ratios) != 3 or abs(sum(split_ratios) - 1.0) > 1e-9:\n",
    "        raise ValueError(\"Split ratios must be a tuple of 3 numbers that sum to 1.0\")\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    random.seed(42)\n",
    "    \n",
    "    # Create dataset directory structure\n",
    "    splits = create_dataset_structure(output_folder)\n",
    "    \n",
    "    # Convert paths to Path objects\n",
    "    image_folder = Path(image_folder)\n",
    "    shapefile_folder = Path(shapefile_folder)\n",
    "    output_folder = Path(output_folder)\n",
    "    \n",
    "    # Track statistics\n",
    "    stats = {\n",
    "        'processed': 0,\n",
    "        'skipped': 0,\n",
    "        'invalid_polygons': 0,\n",
    "        'empty_labels': 0,\n",
    "        'multipart_processed': 0\n",
    "    }\n",
    "\n",
    "    # Process each shapefile\n",
    "    shp_files = list(shapefile_folder.glob(\"*.shp\"))\n",
    "    for shp_file in shp_files:        \n",
    "        # Use shapefile name to find corresponding image\n",
    "        base_name = shp_file.stem\n",
    "        \n",
    "        # Search recursively through image_folder and its subdirectories\n",
    "        matching_images = list(image_folder.rglob(f\"{base_name}.jpg\"))\n",
    "        \n",
    "        if not matching_images:\n",
    "            print(f\"Warning: No matching image for {shp_file}\")\n",
    "            continue\n",
    "        \n",
    "        # Use the first matching image if multiple are found\n",
    "        if len(matching_images) > 1:\n",
    "            print(f\"Warning: Multiple matching images found for {shp_file}. Using {matching_images[0]}\")\n",
    "        \n",
    "        img_path = matching_images[0]\n",
    "        \n",
    "        with rasterio.open(img_path) as src:\n",
    "            width, height = src.width, src.height\n",
    "            transform = src.transform\n",
    "\n",
    "        # Assign to a split\n",
    "        split = assign_split(splits, split_ratios)\n",
    "        \n",
    "        # Define paths for label and image in new structure\n",
    "        label_path = output_folder / 'segment' / 'labels' / split / f\"{base_name}.txt\"\n",
    "        new_img_path = output_folder / 'segment' / 'images' / split / f\"{base_name}.jpg\"\n",
    "        \n",
    "        sf = shapefile.Reader(shp_file)\n",
    "        valid_polygons = False  # Track if file has any valid polygons\n",
    "\n",
    "        # Create temporary label file\n",
    "        temp_label_path = label_path.with_suffix('.tmp')\n",
    "\n",
    "        # Write label file\n",
    "        with open(temp_label_path, 'w') as f:\n",
    "            for shape_record in sf.shapeRecords():\n",
    "                label = int(shape_record.record['cool_roof']) # Assuming the label is stored in the attribute table\n",
    "                geom = shape(shape_record.shape.__geo_interface__)\n",
    "                \n",
    "                # Process the geometry and get valid coordinate strings\n",
    "                valid_coord_strings = process_geometry(geom, transform, width, height)\n",
    "                \n",
    "                # Write valid polygons to file\n",
    "                for coord_str in valid_coord_strings:\n",
    "                    valid_polygons = True\n",
    "                    f.write(f\"{label} {coord_str}\\n\")\n",
    "                    \n",
    "                if isinstance(geom, MultiPolygon) and valid_coord_strings:\n",
    "                    stats['multipart_processed'] += 1\n",
    "        \n",
    "        # Only keep files with valid polygons\n",
    "        if valid_polygons:\n",
    "            temp_label_path.rename(label_path)\n",
    "            shutil.copy2(img_path, new_img_path)\n",
    "            stats['processed'] += 1\n",
    "            print(f\"Processed {base_name} -> {split}\")\n",
    "        else:\n",
    "            temp_label_path.unlink(missing_ok=True)\n",
    "            stats['empty_labels'] += 1\n",
    "            print(f\"Skipping {base_name}: No valid polygons\")\n",
    "    \n",
    "    # Print distribution and statistics summary\n",
    "    print(\"\\nDataset Distribution:\")\n",
    "    for split in splits:\n",
    "        label_files = list((output_folder / 'segment' / 'labels' / split).glob('*.txt'))\n",
    "        image_files = list((output_folder / 'segment' / 'images' / split).glob('*.jpg'))\n",
    "        print(f\"{split}: {len(label_files)} labels, {len(image_files)} images\")\n",
    "    \n",
    "    print(\"\\nProcessing Statistics:\")\n",
    "    print(f\"Successfully processed: {stats['processed']} files\")\n",
    "    print(f\"Skipped (no matching image): {stats['skipped']} files\")\n",
    "    print(f\"Invalid polygons encountered: {stats['invalid_polygons']}\")\n",
    "    print(f\"Files with no valid polygons: {stats['empty_labels']}\")\n",
    "    print(f\"Multipart polygons successfully processed: {stats['multipart_processed']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create the custom dataset\n",
    "Using orthoimages and their corresponding shapefiles, create the custom dataset and split the images between training, validation, and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = \"C:/Users/jdhoc/Desktop/DOT Volunteer Project/data/demo/imgs\"\n",
    "shapefile_folder = \"C:/Users/jdhoc/Desktop/DOT Volunteer Project/data/demo/shps\"\n",
    "output_folder = \"C:/Users/jdhoc/Desktop/DOT Volunteer Project/data/demo/dataset\"\n",
    "\n",
    "# Custom split ratios (must sum to 1.0)\n",
    "split_ratios = (0.7, 0.2, 0.1)  # train, val, test\n",
    "\n",
    "create_dataset(image_folder, shapefile_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the YOLO segmentation model\n",
    "Using the custom dataset, train the model to identify individual instances of roofs and classify their type (cool or warm). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_MODEL = \"yolo11n-seg.pt\"\n",
    "CUSTOM_DATASET = \"C:/Users/jdhoc/Desktop/DOT Volunteer Project/data/demo/dataset/segment/segment.yaml\"\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(PRETRAINED_MODEL)\n",
    "\n",
    "# Train the model\n",
    "results = model.train(data = CUSTOM_DATASET, batch = -1, imgsz = 1024, cache = \"disk\", device = 0, degrees = 90.0, scale = 0.5, flipud = 0.25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
